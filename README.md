# Vanilla Machine Learning Projects


## Purpose of This Repository

This repository is designed to:

- Strengthen **machine learning fundamentals**
- Demonstrate ability to **implement algorithms from first principles**
- Serve as a **learning and reference resource** for classical ML
- Act as a **foundation for advanced ML and deep learning work**
---

## Core Focus

- Implementing ML algorithms **from scratch using NumPy**
- Understanding **mathematics, optimization, and model behavior**
- Clear separation between **theory-first implementations** and **library-based applications**
- Clean, readable, and modular code structure
- One deployed ML application to demonstrate practical workflow

---

## Tech Stack

- Python  
- NumPy  
- Pandas  
- Matplotlib  
- Scikit-learn  
- Streamlit  



---

## Projects Overview

### Mini Linear Algebra Utilities (From Scratch)
**Folder:** `Mini-Linear-Algebra-From-Numpy-Scratch`

- Vector and matrix operations
- Dot products, norms, and projections
- Mathematical groundwork for ML algorithms

---

### Vector Visualization (From Scratch)
**Folder:** `Vector-Visualization-From-Numpy-Scratch`

- Vector operations and transformations
- Projection and geometric interpretation
- Visualization using Matplotlib

---

### Linear Regression (From Scratch)
**Folder:** `Linear-Regression-From-Numpy-Scratch`

- Mean Squared Error (MSE) loss
- Gradient Descent optimization
- Manual weight and bias updates
- Emphasis on optimization dynamics

---

### Logistic Regression (From Scratch)
**Folder:** `Logistic-Regression-From-Numpy-Scratch`

- Binary classification using NumPy
- Sigmoid activation
- Binary Cross-Entropy loss
- Decision boundary interpretation

---

### Support Vector Machine (From Scratch)
**Folder:** `SVM-From-Numpy-Scratch`

- Maximum margin classifier
- Hinge loss
- Regularization concept
- Geometric intuition behind margin maximization

---

### Boosting Algorithms Comparison
**Folder:** `XGBoost-CatBoost-GradientBoosting-Comparison`

- Gradient Boosting
- XGBoost
- CatBoost
- Performance and biasâ€“variance comparison

---

### Random Forest Application (With Deployment)
**Folder:** `Random-Forest-with-Streamlit`

- Random Forest model using scikit-learn
- End-to-end ML pipeline
- Model serialization
- Web interface built with Streamlit

---
### Principal Component Analysis (From Scratch)
**Folder:** `PCA-From-Numpy-Scratch`

- Mean centering
- Covariance matrix computation
- Eigenvalue decomposition
- Dimensionality reduction via variance preservation

---

### K-Nearest Neighbors (From Scratch)
**Folder:** `KNN-From-Numpy-Scratch`

- Distance-based (instance-based) learning
- Euclidean distance computation from scratch
- Majority voting for classification
- Effect of **K** on biasâ€“variance tradeoff
- Importance of **feature scaling** for distance metrics
- Multi-class classification on the Iris dataset

---

### Clustering Algorithms
**Folder:** `Clustering`

- K-Means clustering
- Distance-based grouping
- Cluster visualization
- Practical customer segmentation use-case

---

ðŸ“Œ *This repository represents my journey of understanding machine learning from the ground up.*
