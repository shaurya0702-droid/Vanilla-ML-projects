{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb759792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "\n",
    "# Comparative Performance Analysis of Gradient Boosting, XGBoost, and CatBoost on Cancer dataset . \n",
    "# Boosting means combining multiple weak learners to create a strong learner.and correcting errors of previous models.\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0c29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\shara\\appdata\\roaming\\python\\python313\\site-packages (3.1.2)\n",
      "Requirement already satisfied: catboost in c:\\users\\shara\\appdata\\roaming\\python\\python313\\site-packages (1.2.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shara\\appdata\\roaming\\python\\python313\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.15.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\shara\\appdata\\roaming\\python\\python313\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly->catboost) (9.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost catboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #80% train, 20% test, labels for training and testing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "#accuracy_score - It tells you how many predictions were correct.\n",
    "#confusion_matrix - It shows how many were predicted correctly and incorrectly for each class.[tp,tn,fp,fn]\n",
    "#classification_report -It gives precision    recall  f1-score   support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0453149",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\shara\\OneDrive\\Desktop\\ML ACTS\\ML-Projects\\XG,CAT,GRAD Boosting project 4\\boosting_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.drop(['id'], axis=1) #id no use \n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0}) # in dataset M=malignant=1, B=benign=0 => label encoded 1,0\n",
    "X = df.drop('diagnosis', axis=1) # features\n",
    "y = df['diagnosis'] # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    X, y, test_size=0.2, random_state=42 #random_state=42 is convention(any no. will work), it acts as a seed for the random number generator. \n",
    ")\n",
    "  # This means that every time you run your code, the random number generator will Always split exactly the same here, ensuring same results each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features as large numbers dominate — model becomes biased.\n",
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train) #mean deviation and transform -> z=(x-mean)/std\n",
    "X_test = scaler.transform(X_test) # not fit to avoid DATA LEAKAGE \n",
    "#If your model looks at test data during training, it “cheats” and learns patterns it should not know.\n",
    "#fit = learn  , transform = apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69  2]\n",
      " [ 3 40]]\n",
      "\n",
      "\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97        71\n",
      "           1       0.95      0.93      0.94        43\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=150, learning_rate=0.1) #n_estimators = trees = weak learners\n",
    "# excessive trees -> overfitting , \n",
    "gb.fit(X_train, y_train)\n",
    "y_pred_gb = gb.predict(X_test)#output like :[1, 0, 1, 1, 0, ...]\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_gb))\n",
    "print(\"\\n\\n\\n\",classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69  2]\n",
      " [ 3 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97        71\n",
      "           1       0.95      0.93      0.94        43\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb     #vimp\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    eval_metric='logloss' #Log loss measures: how well the model’s predicted probability matches the true label.\n",
    ")# log loss as we are doing classification and you want probabilities 0.93(logloss extremely small), 0.10(logloss huge)\n",
    "# other options than logloss - error(0/1 classif), auc , rmse and mae(reg), hinge(SVM), merror and mlogloss(multi-class), quantile(outliers)\n",
    "#for multiple loss func in xg , we can write eval_metric=['logloss', 'auc'] etc\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f721aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70  1]\n",
      " [ 2 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98        71\n",
      "           1       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier  #for categorical data handling (category+boosting)\n",
    "cat = CatBoostClassifier(\n",
    "    iterations=150,\n",
    "    learning_rate=0.1,\n",
    "    verbose=0   #verbose controls how much CatBoost prints during training: \n",
    "                 #verbose=0 silences all messages, verbose=10 prints current loss every 10 iterations;\n",
    ")#useful to avoid flooding the console on large datasets or many trees, and it does not affect model performance.\n",
    "\n",
    "cat.fit(X_train, y_train)\n",
    "y_pred_cat = cat.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_cat))\n",
    "print(classification_report(y_test, y_pred_cat))\n",
    "\n",
    "#output confusion matrics \n",
    "#[[70  1]   → Actual 0 (Benign)\n",
    "# [ 2 41]]  → Actual 1 (Malignant)\n",
    "#TN = 70 → correctly predicted Benign  FP = 1 → predicted Malignant but actually Benign \n",
    "#FN = 2 → predicted Benign but actually Malignant  TP = 41 → correctly predicted Malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14cd062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting: 0.956140350877193\n",
      "XGBoost: 0.956140350877193\n",
      "CatBoost: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#comparing accuracies of all three models\n",
    "print(\"Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"XGBoost:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"CatBoost:\", accuracy_score(y_test, y_pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4390b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df6b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bec187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f4e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8109f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11806188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd0d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebd344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_score - It tells you how many predictions were correct.\n",
    "#confusion_matrix - It shows how many were predicted correctly and incorrectly for each class.[tp,tn,fp,fn]\n",
    "#classification_report -It gives precision    recall  f1-score   support\n",
    "\n",
    "\n",
    "#If your model looks at test data during training, it “cheats” and learns patterns it should not know.\n",
    "#fit = learn  , transform = apply\n",
    "\n",
    "\n",
    "\n",
    "#output confusion matrics \n",
    "#[[70  1]   → Actual 0 (Benign)\n",
    "# [ 2 41]]  → Actual 1 (Malignant)\n",
    "#TN = 70 → correctly predicted Benign  FP = 1 → predicted Malignant but actually Benign \n",
    "#FN = 2 → predicted Benign but actually Malignant  TP = 41 → correctly predicted Malignant\n",
    "\n",
    "\n",
    "\n",
    "# other options than logloss - error(0/1 classif), auc , rmse and mae(reg), hinge(SVM), merror and mlogloss(multi-class), quantile(outliers)\n",
    "#for multiple loss func in xg , we can write eval_metric=['logloss', 'auc'] etc\n",
    "\n",
    "\n",
    "                 #verbose=0 silences all messages, verbose=10 prints current loss every 10 iterations;\n",
    "  # This means that every time you run your code, the random number generator will Always split exactly the same here, ensuring same results each time.\n",
    "\n",
    "#useful to avoid flooding the console on large datasets or many trees, and it does not affect model performance.\n",
    "\n",
    "\n",
    "# log loss as we are doing classification and you want probabilities 0.93(logloss extremely small), 0.10(logloss huge)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
